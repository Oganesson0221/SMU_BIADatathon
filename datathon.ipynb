{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.11/site-packages (3.8.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (3.1)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.11/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.11/site-packages (3.8.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (8.3.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.10.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.2.0,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.1.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: langdetect in /opt/anaconda3/lib/python3.11/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: validators in /opt/anaconda3/lib/python3.11/site-packages (0.18.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from validators) (1.16.0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from validators) (5.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib networkx seaborn\n",
    "!pip install pyvis --quiet  # For interactive network graphs\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install langdetect\n",
    "!pip install validators\n",
    "\n",
    "# Qixian: This cell takes around 20 seconds and may need to restart kernel\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Link  \\\n",
      "0  https://edition.cnn.com/2023/09/29/business/st...   \n",
      "1  https://www.channelnewsasia.com/singapore/su-w...   \n",
      "2  https://edition.cnn.com/2023/05/22/tech/meta-f...   \n",
      "3  https://www.channelnewsasia.com/singapore/bill...   \n",
      "4  https://edition.cnn.com/2024/03/05/politics/li...   \n",
      "\n",
      "                                                Text  \n",
      "0  Starbucks violated federal labor law when it i...  \n",
      "1  The first suspect to plead guilty in Singapore...  \n",
      "2  Meta has been fined a record-breaking €1.2 bil...  \n",
      "3  SINGAPORE: A 45-year-old man linked to Singapo...  \n",
      "4  The Department of Education imposed a record $...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1509 entries, 0 to 1508\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Link    1509 non-null   object\n",
      " 1   Text    1509 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 23.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Provide the path to your Excel file\n",
    "file_path = \"news_excerpts_parsed.xlsx\"\n",
    "\n",
    "# Load the dataset\n",
    "df_original = pd.read_excel(file_path)\n",
    "df = df_original.copy()\n",
    "\n",
    "# View the first few rows\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Link  \\\n",
      "0  https://edition.cnn.com/2023/09/29/business/st...   \n",
      "1  https://www.channelnewsasia.com/singapore/su-w...   \n",
      "2  https://edition.cnn.com/2023/05/22/tech/meta-f...   \n",
      "3  https://www.channelnewsasia.com/singapore/bill...   \n",
      "4  https://edition.cnn.com/2024/03/05/politics/li...   \n",
      "\n",
      "                                                Text  \n",
      "0  Starbucks violated federal labor law when it i...  \n",
      "1  The first suspect to plead guilty in Singapore...  \n",
      "2  Meta has been fined a record-breaking €1.2 bil...  \n",
      "3  SINGAPORE: A 45-year-old man linked to Singapo...  \n",
      "4  The Department of Education imposed a record $...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1509 entries, 0 to 1508\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Link    1509 non-null   object\n",
      " 1   Text    1509 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 23.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = df_original.copy()\n",
    "\n",
    "print(df.head())  # View the first few rows\n",
    "print(df.info())  # Check for missing or null valuesdf = df_original.copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Validation Summary:\n",
      "Total Rows Initially: 1509\n",
      "Rows Removed (Missing Content): 0\n",
      "Rows Removed (Empty Content): 0\n",
      "Rows Removed (Duplicates): 0\n",
      "Rows Removed (Invalid Length): 0\n",
      "Final Row Count: 1509\n",
      "Total Rows Removed: 0\n"
     ]
    }
   ],
   "source": [
    "def validate_data(dataframe):\n",
    "    original_rows = len(dataframe)\n",
    "\n",
    "    # Check for missing values\n",
    "    dataframe = dataframe.dropna(subset=['Text'])\n",
    "    rows_removed_missing = original_rows - len(dataframe)\n",
    "\n",
    "    # Check for empty strings or whitespaces\n",
    "    dataframe['content_length'] = dataframe['Text'].apply(lambda x: len(str(x).strip()))\n",
    "    dataframe = dataframe[dataframe['content_length'] > 0]\n",
    "    rows_removed_empty = original_rows - len(dataframe) - rows_removed_missing\n",
    "\n",
    "    # Remove duplicates\n",
    "    original_rows = len(dataframe)\n",
    "    dataframe = dataframe.drop_duplicates(subset=['Text'], keep='first')\n",
    "    rows_removed_duplicates = original_rows - len(dataframe)\n",
    "\n",
    "    # Check for invalid lengths\n",
    "    min_length, max_length = 50, 10000\n",
    "    # Use 'content_length' instead of 'Text_length' for filtering\n",
    "    dataframe = dataframe[(dataframe['content_length'] >= min_length) & (dataframe['content_length'] <= max_length)]\n",
    "    rows_removed_length = original_rows - len(dataframe)\n",
    "\n",
    "    # Final count\n",
    "    final_rows = len(dataframe)\n",
    "    rows_removed_total = original_rows - final_rows\n",
    "\n",
    "    # Summary\n",
    "    print(\"Data Validation Summary:\")\n",
    "    print(f\"Total Rows Initially: {original_rows}\")\n",
    "    print(f\"Rows Removed (Missing Content): {rows_removed_missing}\")\n",
    "    print(f\"Rows Removed (Empty Content): {rows_removed_empty}\")\n",
    "    print(f\"Rows Removed (Duplicates): {rows_removed_duplicates}\")\n",
    "    print(f\"Rows Removed (Invalid Length): {rows_removed_length}\")\n",
    "    print(f\"Final Row Count: {final_rows}\")\n",
    "    print(f\"Total Rows Removed: {rows_removed_total}\")\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "df = validate_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for non-English content...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 1509/1509 [00:03<00:00, 435.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating URL format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 1509/1509 [00:00<00:00, 106274.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accessibility for invalid format links...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional Validation Summary:\n",
      "Total Rows Initially: 1509\n",
      "Rows Removed (Non-English Content): 0\n",
      "Rows Removed (Invalid Format and Inaccessible Links): 0\n",
      "Final Row Count After Additional Checks: 1509\n",
      "Total Rows Removed in Additional Checks: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import validators\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "\n",
    "def additional_checks_with_progress(dataframe):\n",
    "    \"\"\"\n",
    "    Perform additional checks for non-English content and validate URLs.\n",
    "    Perform accessibility checks only on links with invalid format and remove inaccessible ones.\n",
    "    \"\"\"\n",
    "    original_rows = len(dataframe)  # Initial row count\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    tqdm.pandas(desc=\"Processing rows\")\n",
    "\n",
    "    # 1. Detect Non-English Content\n",
    "    print(\"Checking for non-English content...\")\n",
    "    def detect_language_with_progress(text):\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except:\n",
    "            return \"error\"\n",
    "\n",
    "    dataframe['language'] = dataframe['Text'].progress_apply(detect_language_with_progress)\n",
    "    non_english_rows = dataframe[dataframe['language'] != 'en']  # Identify non-English rows\n",
    "    dataframe = dataframe[dataframe['language'] == 'en']  # Keep only English rows\n",
    "    rows_removed_non_english = len(non_english_rows)\n",
    "\n",
    "    # 2. Validate URLs in the 'Link' column\n",
    "    print(\"Validating URL format...\")\n",
    "    def is_format_valid(url):\n",
    "        \"\"\"\n",
    "        Validates the URL format using validators.\n",
    "        Returns True if the format is valid, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return validators.url(url.strip()) is True  # Strip whitespace before validation\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    dataframe['valid_format'] = dataframe['Link'].progress_apply(is_format_valid)\n",
    "\n",
    "    # Log invalid format links\n",
    "    invalid_format_links = dataframe[~dataframe['valid_format']]  # Links with invalid format\n",
    "    if not invalid_format_links.empty:\n",
    "        print(\"\\nLinks with Invalid Format:\")\n",
    "        print(invalid_format_links['Link'].tolist())\n",
    "\n",
    "    # 3. Perform Accessibility Check on Invalid Format Links\n",
    "    print(\"Checking accessibility for invalid format links...\")\n",
    "    def is_accessible(url):\n",
    "        \"\"\"\n",
    "        Checks accessibility of a URL using a GET request with headers.\n",
    "        Returns True if the URL is accessible, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Normalize the URL by stripping spaces and adding http:// if missing\n",
    "            url = url.strip()\n",
    "            if not url.startswith((\"http://\", \"https://\")):\n",
    "                url = \"http://\" + url\n",
    "\n",
    "            headers = {\n",
    "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=5)\n",
    "            return response.status_code < 400  # Accessible if status code is OK\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    # Perform accessibility check only on invalid format links\n",
    "    invalid_format_links['accessible'] = invalid_format_links['Link'].progress_apply(is_accessible)\n",
    "\n",
    "    # Log inaccessible links\n",
    "    inaccessible_links = invalid_format_links[~invalid_format_links['accessible']]\n",
    "    if not inaccessible_links.empty:\n",
    "        print(\"\\nInaccessible Links Removed:\")\n",
    "        print(inaccessible_links['Link'].tolist())\n",
    "\n",
    "    # Remove inaccessible links\n",
    "    valid_invalid_links = invalid_format_links[invalid_format_links['accessible']]\n",
    "    dataframe = pd.concat([dataframe[dataframe['valid_format']], valid_invalid_links])\n",
    "\n",
    "    # Final row count\n",
    "    final_rows = len(dataframe)\n",
    "    rows_removed_invalid_links = len(inaccessible_links)\n",
    "    rows_removed_total = original_rows - final_rows\n",
    "\n",
    "    # Print the additional validation summary\n",
    "    print(\"\\nAdditional Validation Summary:\")\n",
    "    print(f\"Total Rows Initially: {original_rows}\")\n",
    "    print(f\"Rows Removed (Non-English Content): {rows_removed_non_english}\")\n",
    "    print(f\"Rows Removed (Invalid Format and Inaccessible Links): {rows_removed_invalid_links}\")\n",
    "    print(f\"Final Row Count After Additional Checks: {final_rows}\")\n",
    "    print(f\"Total Rows Removed in Additional Checks: {rows_removed_total}\")\n",
    "\n",
    "    # Return the cleaned dataframe\n",
    "    return dataframe\n",
    "\n",
    "# Apply the additional checks with progress bar\n",
    "df = additional_checks_with_progress(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Entities: 100%|██████████| 1509/1509 [00:22<00:00, 65.86it/s]\n",
      "Extracting Relationships: 100%|██████████| 1509/1509 [00:21<00:00, 71.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  \\\n",
      "0  Starbucks violated federal labor law when it i...   \n",
      "1  The first suspect to plead guilty in Singapore...   \n",
      "2  Meta has been fined a record-breaking €1.2 bil...   \n",
      "3  SINGAPORE: A 45-year-old man linked to Singapo...   \n",
      "4  The Department of Education imposed a record $...   \n",
      "\n",
      "                                            entities  \\\n",
      "0  [(National Labor Relations Board, ORG), (Thurs...   \n",
      "1  [(first, ORDINAL), (Singapore, GPE), (13 month...   \n",
      "2  [(Meta, ORG), (€1.2 billion, MONEY), ($1.3 bil...   \n",
      "3  [(SINGAPORE, GPE), (45-year-old, DATE), (Singa...   \n",
      "4  [(The Department of Education, ORG), (a record...   \n",
      "\n",
      "                                       relationships  \n",
      "0  [(Starbucks, violated, 1), (law, violated, 1),...  \n",
      "1  [(Wenqiang, admitted, 39), (proceeds, possessi...  \n",
      "2  [(billion, fined, 3), (laws, violating, 21), (...  \n",
      "3        [(This, amounts, 50), (Zhang, pleaded, 90)]  \n",
      "4  [(Department, imposed, 4), (fine, imposed, 4),...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "# Initialize SpaCy model and TQDM progress bar\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tqdm.pandas(desc=\"Extracting Entities\")\n",
    "\n",
    "# Define the function for entity extraction\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Apply the function with a progress bar\n",
    "df['entities'] = df['Text'].progress_apply(extract_entities)\n",
    "\n",
    "# Define a function for extracting relationships using dependency parsing\n",
    "def extract_relationships(text):\n",
    "    \"\"\"\n",
    "    Extracts relationships between entities based on dependency parsing.\n",
    "    Returns a list of (subject, verb, object) tuples.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    relationships = []\n",
    "    for token in doc:\n",
    "        if token.dep_ in (\"nsubj\", \"dobj\") and token.head.pos_ == \"VERB\":\n",
    "            relationships.append((token.text, token.head.text, token.head.i))\n",
    "    return relationships\n",
    "\n",
    "# Add a progress bar for the relationship extraction\n",
    "tqdm.pandas(desc=\"Extracting Relationships\")\n",
    "df['relationships'] = df['Text'].progress_apply(extract_relationships)\n",
    "\n",
    "# Display a few rows of the DataFrame with entities and relationships\n",
    "print(df[['Text', 'entities', 'relationships']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entities: 21323\n",
      "Total number of relationships: 20062\n"
     ]
    }
   ],
   "source": [
    "total_entities = df['entities'].apply(len).sum()\n",
    "total_relationships = df['relationships'].apply(len).sum()\n",
    "\n",
    "print(f\"Total number of entities: {total_entities}\")\n",
    "print(f\"Total number of relationships: {total_relationships}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entity_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Add entities as nodes\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ent, label \u001b[38;5;129;01min\u001b[39;00m entities:\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mentity_graph\u001b[49m\u001b[38;5;241m.\u001b[39madd_node(ent, label\u001b[38;5;241m=\u001b[39mlabel)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Add relationships as edges\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subj, verb, obj_token \u001b[38;5;129;01min\u001b[39;00m relationships:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entity_graph' is not defined"
     ]
    }
   ],
   "source": [
    "# Build the entity-relationship graph\n",
    "for _, row in df.iterrows():\n",
    "    entities = row['entities']\n",
    "    relationships = row['relationships']\n",
    "\n",
    "    # Add entities as nodes\n",
    "    for ent, label in entities:\n",
    "        entity_graph.add_node(ent, label=label)\n",
    "\n",
    "    # Add relationships as edges\n",
    "    for subj, verb, obj_token in relationships:\n",
    "        obj = obj_token.text\n",
    "        if subj in entity_graph and obj in entity_graph:\n",
    "            entity_graph.add_edge(subj, obj, relation=verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
